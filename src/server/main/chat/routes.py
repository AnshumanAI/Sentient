# src/server/main/chat/routes.py
import datetime
import uuid
import json
import asyncio
from fastapi import APIRouter, Depends
from fastapi.responses import JSONResponse, StreamingResponse

from .models import ChatMessageInput
from .utils import get_chat_history_util, generate_chat_llm_stream
from ..auth.utils import PermissionChecker # Using auth.utils for PermissionChecker
from ..app import mongo_manager_instance as mongo_manager # Global instance
from ..app import auth_helper # Global instance
from ..app import is_dev_env # Global flag
# from ..app import ollama_llm, openrouter_llm # If LLM runnables are global

router = APIRouter(
    tags=["Chat"]
)

@router.post("/get-history", summary="Get Chat History")
async def get_chat_history_endpoint(user_id: str = Depends(PermissionChecker(required_permissions=["read:chat"]))):
    # Pass the global mongo_manager instance
    messages, active_chat_id = await get_chat_history_util(user_id, mongo_manager)
    return JSONResponse(content={"messages": messages, "activeChatId": active_chat_id})

@router.post("/clear-chat-history", summary="Clear Active Chat History")
async def clear_chat_history_endpoint(user_id: str = Depends(PermissionChecker(required_permissions=["write:chat"]))):
    user_profile = await mongo_manager.get_user_profile(user_id)
    active_chat_id = user_profile.get("userData", {}).get("active_chat_id") if user_profile else None
    
    if active_chat_id:
        await mongo_manager.delete_chat_history(user_id, active_chat_id)
    
    # Always assign a new active chat ID after clearing, or if none existed
    new_active_chat_id = str(uuid.uuid4())
    await mongo_manager.update_user_profile(user_id, {"userData.active_chat_id": new_active_chat_id})
    # Ensure a chat document is created for this new ID if FE expects it immediately
    # (or let get_chat_history_util handle it on next load)
    
    return JSONResponse(content={"message": "Chat history cleared.", "activeChatId": new_active_chat_id})


@router.post("/chat", summary="Process Chat Message (Text)")
async def chat_endpoint(
    request_body: ChatMessageInput, 
    user_id: str = Depends(PermissionChecker(required_permissions=["read:chat", "write:chat"]))
):
    user_profile = await mongo_manager.get_user_profile(user_id)
    username = user_profile.get("userData", {}).get("personalInfo", {}).get("name", user_id) if user_profile else user_id
    
    # Get active chat ID, this util also ensures one exists and is set in profile
    _, active_chat_id = await get_chat_history_util(user_id, mongo_manager)

    user_msg_id = await mongo_manager.add_chat_message(user_id, active_chat_id, {
        "message": request_body.input, "isUser": True, "isVisible": True 
        # "id" will be generated by add_chat_message if not provided
    })

    async def response_generator():
        yield json.dumps({
            "type": "userMessage", "id": user_msg_id, "message": request_body.input, 
            "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat()
        }) + "\n"
        
        assistant_temp_id_for_stream = str(uuid.uuid4()) # ID for the streaming message
        yield json.dumps({"type": "intermediary", "message": "Thinking...", "id": assistant_temp_id_for_stream}) + "\n"
        await asyncio.sleep(0.1)

        full_llm_response_text = ""
        final_message_id_from_llm_stream = assistant_temp_id_for_stream # Default to this if not overridden by stream
        
        # In a real setup, you'd pass configured LLM runnable instances here
        # e.g. from ..app import ollama_llm, openrouter_llm
        async for item in generate_chat_llm_stream(
            request_body.input, username, is_dev_env #, ollama_llm, openrouter_llm
            ):
            full_llm_response_text += item.get("token", "")
            final_message_id_from_llm_stream = item.get("messageId", final_message_id_from_llm_stream)
            yield json.dumps(item) + "\n"
        
        # Store the complete assistant message with the ID used during streaming
        await mongo_manager.add_chat_message(user_id, active_chat_id, {
            "id": final_message_id_from_llm_stream, # Use the ID from the stream
            "message": full_llm_response_text, 
            "isUser": False, 
            "isVisible": True,
            # Include any other metadata from the LLM stream if available
            "memoryUsed": False, "agentsUsed": False, "internetUsed": False, "proUsed": False 
        })
            
    return StreamingResponse(response_generator(), media_type="application/x-ndjson")